{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "model = models.vgg19(pretrained=True).features.to(device)\n",
        "print(model)\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.chosen_layers = ['0', '5', '10', '19', '28']\n",
        "        self.model = models.vgg19(pretrained=True).features[:29]\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for layer_num, layer in enumerate(self.model):\n",
        "            x = layer(x)\n",
        "            print(f\"After layer {layer_num}, shape: {x.shape}\")\n",
        "            if str(layer_num) in self.chosen_layers:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "def load_image(image_name):\n",
        "        image = Image.open(image_name)\n",
        "        image = loader(image).unsqueeze(0)\n",
        "        print(\"Loaded image shape:\", image.shape)\n",
        "        return image.to(device)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_size = 356\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "original_img = load_image('/content/Man.jpg')\n",
        "style_img = load_image('/content/picasso1.jpg')\n",
        "\n",
        "generated = original_img.clone().requires_grad_(True)\n",
        "\n",
        "toal_steps = 60000\n",
        "learning_rate = 0.001\n",
        "alpha = 1\n",
        "beta = 0.001\n",
        "optimizer = optim.Adam([generated], lr=learning_rate)\n",
        "\n",
        "for step in range(toal_steps):\n",
        "    generated_features = model(generated)\n",
        "    original_img_features = model(original_img)\n",
        "    style_features = model(style_img)\n",
        "\n",
        "    style_loss = original_loss = 0\n",
        "\n",
        "    for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_features):\n",
        "        for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_features):\n",
        "            if len(gen_feature.shape) == 3:\n",
        "                gen_feature = gen_feature.unsqueeze(0)\n",
        "            if len(orig_feature.shape) == 3:\n",
        "                orig_feature = orig_feature.unsqueeze(0)\n",
        "            if len(style_feature.shape) == 3:\n",
        "                style_feature = style_feature.unsqueeze(0)\n",
        "        batch_size, channel, height, width = gen_feature.shape\n",
        "        original_loss += torch.mean((gen_feature - orig_feature) ** 2)\n",
        "\n",
        "        G = gen_feature.view(channel, height * width).mm(gen_feature.view(channel, height * width).t())\n",
        "\n",
        "        A = style_feature.view(channel, height * width).mm(style_feature.view(channel, height * width).t())\n",
        "        style_loss += torch.mean((G - A) ** 2)\n",
        "\n",
        "        total_loss = alpha * original_loss + beta * style_loss\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(\"Step: \", step, \"Total Loss: \", total_loss.item())\n",
        "            save_image(generated, \"generated.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XwPZ1BECYhY",
        "outputId": "c0baa7aa-bdb9-4f1c-efba-6bad9cd9db6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): ReLU(inplace=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (26): ReLU(inplace=True)\n",
            "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (31): ReLU(inplace=True)\n",
            "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (33): ReLU(inplace=True)\n",
            "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (35): ReLU(inplace=True)\n",
            "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "Loaded image shape: torch.Size([1, 3, 356, 356])\n",
            "Loaded image shape: torch.Size([1, 3, 356, 356])\n",
            "Step:  0 Total Loss:  0.2127569168806076\n",
            "Step:  200 Total Loss:  0.15071168541908264\n",
            "Step:  400 Total Loss:  0.14961324632167816\n",
            "Step:  600 Total Loss:  0.14891590178012848\n",
            "Step:  800 Total Loss:  0.1480606347322464\n",
            "Step:  1000 Total Loss:  0.14702108502388\n",
            "Step:  1200 Total Loss:  0.14567291736602783\n",
            "Step:  1400 Total Loss:  0.1449793577194214\n",
            "Step:  1600 Total Loss:  0.14456917345523834\n",
            "Step:  1800 Total Loss:  0.14422164857387543\n",
            "Step:  2000 Total Loss:  0.14387434720993042\n",
            "Step:  2200 Total Loss:  0.14351658523082733\n",
            "Step:  2400 Total Loss:  0.14312000572681427\n",
            "Step:  2600 Total Loss:  0.14285680651664734\n",
            "Step:  2800 Total Loss:  0.14262743294239044\n",
            "Step:  3000 Total Loss:  0.1424649953842163\n",
            "Step:  3200 Total Loss:  0.1422707885503769\n",
            "Step:  3400 Total Loss:  0.14211918413639069\n",
            "Step:  3600 Total Loss:  0.14198841154575348\n",
            "Step:  3800 Total Loss:  0.1418847143650055\n",
            "Step:  4000 Total Loss:  0.1417754888534546\n",
            "Step:  4200 Total Loss:  0.141665518283844\n",
            "Step:  4400 Total Loss:  0.14158204197883606\n",
            "Step:  4600 Total Loss:  0.1415000557899475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a8lJgoqRCdLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzMwendlCSqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmf8p4sJCIi_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5dI3EczCskI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIxPZRaGCyCM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}